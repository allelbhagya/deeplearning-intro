#### implementation of a dense layer

```python
import tensorflow as tf

class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, input_dim, output_dim):
        super(MyDenseLayer, self).__init__()

        # initializing weights and bias
        self.W = self.add_weight([input_dim, output_dim])
        self.b = self.add_weight([1, output_dim])
    
    def call(self, inputs):

        # forward propagate the inputs
        z = tf.matmul(inputs, self.W) + self.b
        #feed through a non-linear activation function
        output = tf.math.sigmoid(z)
        
        return output
``` 

#### empirical loss

```python
import tensorflow as tf

loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, predicted))

loss_ = tf.reduce_mean(tf.square(tf.subtract(y, predicted)))
loss_ = tf.keras.losses.MSE(y, predicted)

```

#### gradient descent

```python
import tensorflow as tf

weights = tf.Variable([tf.random.normal()])

while True:
    with tf.GradientTape() as g:
        loss = compute_loss(weights)
        gradient = g.gradient(loss,weights)
    weights = weights - lr * gradient
```

#### gradient descent algorithims

```python
sdg = tf.keras.optimizers.SGD
adam = tf.keras.optimizers.Adam
adadelta = tf.keras.optimizers.Adadelta
adagrad = tf.keras.optimizers.Adagrad
rmsprop = tf.keras.optimizers.RMSProp
```

together

```python
import tensorflow as tf

model = tf.keras.Sequential([...])

#picking your optimizer
optimizer = tf.keras.optimizer.SGD()

while True:
    prediction = model(x)
    #forward pass through the network
    with tf.GradientTape() as tape:
        #compute the loss
        loss = compute_loss(y, prediction)
    #update the weights using the gradient
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
```